"""
Parquet åˆ†åŒºç»ƒä¹ æ¨¡å—

æä¾›åˆ†åŒºè¡¨çš„åˆ›å»ºã€æŸ¥è¯¢å’Œæ€§èƒ½åˆ†æåŠŸèƒ½ã€‚
"""

import pyarrow as pa
import pyarrow.parquet as pq
import pandas as pd
import os
import shutil
from typing import Dict, Any, List, Optional

from .utils import PerformanceAnalyzer


class ParquetPartitioningExercise:
    """Parquet åˆ†åŒºç»ƒä¹ ç±»"""
    
    def __init__(self, data_df: pd.DataFrame, output_dir: str = "output"):
        """
        åˆå§‹åŒ–åˆ†åŒºç»ƒä¹ 
        
        Args:
            data_df: è¦æµ‹è¯•çš„æ•°æ®
            output_dir: è¾“å‡ºç›®å½•
        """
        self.df = data_df
        self.output_dir = output_dir
        self.performance_analyzer = PerformanceAnalyzer()
        
        # åˆ†åŒºå’Œéåˆ†åŒºè¡¨çš„è·¯å¾„
        self.non_partitioned_path = os.path.join(output_dir, 'non_partitioned.parquet')
        self.partitioned_path = os.path.join(output_dir, 'partitioned_table')
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
    
    def create_non_partitioned_table(self) -> None:
        """åˆ›å»ºéåˆ†åŒºè¡¨"""
        print("åˆ›å»ºéåˆ†åŒºè¡¨...")
        table = pa.Table.from_pandas(self.df)
        pq.write_table(table, self.non_partitioned_path)
        
        file_size = self.performance_analyzer.get_file_size(self.non_partitioned_path)
        print(f"éåˆ†åŒºè¡¨å·²åˆ›å»º: {self.non_partitioned_path}")
        print(f"æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
    
    def create_partitioned_table(self, partition_cols: List[str] = None) -> None:
        """
        åˆ›å»ºåˆ†åŒºè¡¨
        
        Args:
            partition_cols: åˆ†åŒºåˆ—ï¼Œé»˜è®¤æŒ‰åŸå¸‚åˆ†åŒº
        """
        if partition_cols is None:
            partition_cols = ['City']
        
        print(f"åˆ›å»ºåˆ†åŒºè¡¨ï¼ˆæŒ‰ {', '.join(partition_cols)} åˆ†åŒºï¼‰...")
        
        # æ¸…ç†å·²å­˜åœ¨çš„åˆ†åŒºè¡¨
        if os.path.exists(self.partitioned_path):
            shutil.rmtree(self.partitioned_path)
        
        table = pa.Table.from_pandas(self.df)
        
        # å†™å…¥åˆ†åŒºè¡¨ - ä½¿ç”¨ dataset API ä¿ç•™åˆ†åŒºåˆ—
        import pyarrow.dataset as ds
        partitioning = ds.partitioning(
            pa.schema([pa.field(col, pa.string()) for col in partition_cols]),
            flavor="hive"
        )
        ds.write_dataset(
            table,
            base_dir=self.partitioned_path,
            format="parquet",
            partitioning=partitioning
        )
        
        # ç»Ÿè®¡åˆ†åŒºä¿¡æ¯
        partition_info = self._analyze_partitions()
        print(f"åˆ†åŒºè¡¨å·²åˆ›å»º: {self.partitioned_path}")
        print(f"åˆ†åŒºæ•°é‡: {partition_info['partition_count']}")
        print(f"æ€»å¤§å°: {partition_info['total_size']:.2f} MB")
        
        return partition_info
    
    def _analyze_partitions(self) -> Dict[str, Any]:
        """åˆ†æåˆ†åŒºä¿¡æ¯"""
        partition_count = 0
        total_size = 0
        partition_details = []
        
        for root, dirs, files in os.walk(self.partitioned_path):
            for file in files:
                if file.endswith('.parquet'):
                    file_path = os.path.join(root, file)
                    size = self.performance_analyzer.get_file_size(file_path)
                    total_size += size
                    partition_count += 1
                    
                    # æå–åˆ†åŒºä¿¡æ¯
                    rel_path = os.path.relpath(root, self.partitioned_path)
                    partition_details.append({
                        'path': rel_path,
                        'file': file,
                        'size_mb': size
                    })
        
        return {
            'partition_count': partition_count,
            'total_size': total_size,
            'partitions': partition_details
        }
    
    def test_partition_pruning(self, filter_city: str = "Beijing") -> Dict[str, Any]:
        """
        æµ‹è¯•åˆ†åŒºè£å‰ª
        
        Args:
            filter_city: è¦è¿‡æ»¤çš„åŸå¸‚
            
        Returns:
            åˆ†åŒºè£å‰ªæµ‹è¯•ç»“æœ
        """
        print("=" * 60)
        print("æµ‹è¯•åˆ†åŒºè£å‰ª")
        print("=" * 60)
        
        # æµ‹è¯•éåˆ†åŒºè¡¨æŸ¥è¯¢
        def query_non_partitioned():
            table = pq.read_table(self.non_partitioned_path)
            df = table.to_pandas()
            return df[df['City'] == filter_city]
        
        df_non_part, time_non_part = self.performance_analyzer.measure_time(query_non_partitioned)
        
        print(f"éåˆ†åŒºè¡¨æŸ¥è¯¢ (åŸå¸‚={filter_city}): {time_non_part:.4f} ç§’")
        print(f"ç»“æœè¡Œæ•°: {len(df_non_part)}")
        
        # æµ‹è¯•åˆ†åŒºè¡¨æŸ¥è¯¢
        def query_partitioned():
            import pyarrow.dataset as ds
            # è¯»å–æ‰€æœ‰æ•°æ®ï¼Œåˆ†åŒºä¿¡æ¯ä¼šè‡ªåŠ¨æ·»åŠ åˆ°ç»“æœä¸­
            dataset = ds.dataset(self.partitioned_path, format="parquet")
            table = dataset.to_table()
            df = table.to_pandas()
            # æ‰‹åŠ¨è¿‡æ»¤ï¼Œå› ä¸ºåˆ†åŒºåˆ—åœ¨è¿™ç§æƒ…å†µä¸‹ä¸åœ¨ schema ä¸­
            return df[df['City'] == filter_city] if 'City' in df.columns else df
        
        df_part, time_part = self.performance_analyzer.measure_time(query_partitioned)
        
        print(f"åˆ†åŒºè¡¨æŸ¥è¯¢ (åŸå¸‚={filter_city}): {time_part:.4f} ç§’")
        print(f"ç»“æœè¡Œæ•°: {len(df_part)}")
        
        speedup = time_non_part / time_part if time_part > 0 else 0
        print(f"æ€§èƒ½æå‡: {speedup:.2f}x")
        
        # éªŒè¯ç»“æœä¸€è‡´æ€§
        data_consistent = len(df_non_part) == len(df_part)
        print(f"æ•°æ®ä¸€è‡´æ€§: {'âœ“' if data_consistent else 'âœ—'}")
        
        return {
            'non_partitioned_time': time_non_part,
            'partitioned_time': time_part,
            'speedup': speedup,
            'result_rows': len(df_part),
            'data_consistent': data_consistent
        }
    
    def test_multiple_partition_queries(self) -> Dict[str, Any]:
        """
        æµ‹è¯•å¤šç§åˆ†åŒºæŸ¥è¯¢åœºæ™¯
        
        Returns:
            å¤šç§æŸ¥è¯¢åœºæ™¯çš„æµ‹è¯•ç»“æœ
        """
        print("\n" + "=" * 60)
        print("æµ‹è¯•å¤šç§åˆ†åŒºæŸ¥è¯¢åœºæ™¯")
        print("=" * 60)
        
        results = {}
        
        # è·å–æ‰€æœ‰åŸå¸‚
        cities = self.df['City'].unique()
        
        # åœºæ™¯1ï¼šå•åˆ†åŒºæŸ¥è¯¢
        print("åœºæ™¯1: å•åˆ†åŒºæŸ¥è¯¢")
        test_city = cities[0] if len(cities) > 0 else "Beijing"
        single_result = self.test_single_partition_query(test_city)
        results['single_partition'] = single_result
        
        # åœºæ™¯2ï¼šå¤šåˆ†åŒºæŸ¥è¯¢
        print("\nåœºæ™¯2: å¤šåˆ†åŒºæŸ¥è¯¢")
        test_cities = cities[:3] if len(cities) >= 3 else cities
        multi_result = self.test_multi_partition_query(test_cities)
        results['multi_partition'] = multi_result
        
        # åœºæ™¯3ï¼šå…¨è¡¨æ‰«æ
        print("\nåœºæ™¯3: å…¨è¡¨æ‰«æ")
        full_result = self.test_full_scan()
        results['full_scan'] = full_result
        
        return results
    
    def test_single_partition_query(self, city: str) -> Dict[str, Any]:
        """æµ‹è¯•å•åˆ†åŒºæŸ¥è¯¢"""
        def query_single_partition():
            import pyarrow.dataset as ds
            dataset = ds.dataset(self.partitioned_path, format="parquet")
            table = dataset.to_table()
            df = table.to_pandas()
            return df[df['City'] == city] if 'City' in df.columns else df
        
        df_result, query_time = self.performance_analyzer.measure_time(query_single_partition)
        
        print(f"æŸ¥è¯¢åŸå¸‚ '{city}': {query_time:.4f} ç§’, ç»“æœ: {len(df_result)} è¡Œ")
        
        return {
            'city': city,
            'time': query_time,
            'rows': len(df_result)
        }
    
    def test_multi_partition_query(self, cities: List[str]) -> Dict[str, Any]:
        """æµ‹è¯•å¤šåˆ†åŒºæŸ¥è¯¢"""
        def query_multi_partitions():
            import pyarrow.dataset as ds
            dataset = ds.dataset(self.partitioned_path, format="parquet")
            table = dataset.to_table()
            df = table.to_pandas()
            return df[df['City'].isin(cities)] if 'City' in df.columns else df
        
        df_result, query_time = self.performance_analyzer.measure_time(query_multi_partitions)
        
        print(f"æŸ¥è¯¢åŸå¸‚ {cities}: {query_time:.4f} ç§’, ç»“æœ: {len(df_result)} è¡Œ")
        
        return {
            'cities': cities,
            'time': query_time,
            'rows': len(df_result)
        }
    
    def test_full_scan(self) -> Dict[str, Any]:
        """æµ‹è¯•å…¨è¡¨æ‰«æ"""
        def query_full_table():
            dataset = pq.ParquetDataset(self.partitioned_path)
            table = dataset.read()
            return table.to_pandas()
        
        df_result, query_time = self.performance_analyzer.measure_time(query_full_table)
        
        print(f"å…¨è¡¨æ‰«æ: {query_time:.4f} ç§’, ç»“æœ: {len(df_result)} è¡Œ")
        
        return {
            'time': query_time,
            'rows': len(df_result)
        }
    
    def analyze_partition_distribution(self) -> Dict[str, Any]:
        """
        åˆ†æåˆ†åŒºæ•°æ®åˆ†å¸ƒ
        
        Returns:
            åˆ†åŒºåˆ†å¸ƒåˆ†æç»“æœ
        """
        print("\n" + "=" * 60)
        print("åˆ†æåˆ†åŒºæ•°æ®åˆ†å¸ƒ")
        print("=" * 60)
        
        # ç»Ÿè®¡æ¯ä¸ªåŸå¸‚çš„æ•°æ®é‡
        city_counts = self.df['City'].value_counts()
        
        print("å„åŸå¸‚æ•°æ®åˆ†å¸ƒ:")
        for city, count in city_counts.items():
            percentage = (count / len(self.df)) * 100
            print(f"â€¢ {city}: {count:,} è¡Œ ({percentage:.1f}%)")
        
        # åˆ†æåˆ†åŒºå¤§å°
        partition_info = self._analyze_partitions()
        
        print(f"\nåˆ†åŒºæ–‡ä»¶åˆ†å¸ƒ:")
        for partition in partition_info['partitions']:
            print(f"â€¢ {partition['path']}: {partition['size_mb']:.2f} MB")
        
        # è®¡ç®—åˆ†åŒºå‡è¡¡æ€§
        sizes = [p['size_mb'] for p in partition_info['partitions']]
        if sizes:
            avg_size = sum(sizes) / len(sizes)
            max_size = max(sizes)
            min_size = min(sizes)
            balance_ratio = min_size / max_size if max_size > 0 else 0
            
            print(f"\nåˆ†åŒºå‡è¡¡æ€§åˆ†æ:")
            print(f"â€¢ å¹³å‡å¤§å°: {avg_size:.2f} MB")
            print(f"â€¢ æœ€å¤§åˆ†åŒº: {max_size:.2f} MB")
            print(f"â€¢ æœ€å°åˆ†åŒº: {min_size:.2f} MB")
            print(f"â€¢ å‡è¡¡æ¯”ä¾‹: {balance_ratio:.2f}")
        
        return {
            'city_distribution': city_counts.to_dict(),
            'partition_info': partition_info,
            'balance_metrics': {
                'avg_size': avg_size if sizes else 0,
                'max_size': max_size if sizes else 0,
                'min_size': min_size if sizes else 0,
                'balance_ratio': balance_ratio if sizes else 0
            }
        }
    
    def test_nested_partitioning(self, partition_cols: List[str] = None) -> Dict[str, Any]:
        """
        æµ‹è¯•åµŒå¥—åˆ†åŒº
        
        Args:
            partition_cols: å¤šçº§åˆ†åŒºåˆ—
            
        Returns:
            åµŒå¥—åˆ†åŒºæµ‹è¯•ç»“æœ
        """
        if partition_cols is None:
            partition_cols = ['City', 'AgeGroup']
        
        print("\n" + "=" * 60)
        print(f"æµ‹è¯•åµŒå¥—åˆ†åŒº ({' -> '.join(partition_cols)})")
        print("=" * 60)
        
        # æ·»åŠ å¹´é¾„æ®µåˆ—ç”¨äºåµŒå¥—åˆ†åŒº
        df_with_age_group = self.df.copy()
        df_with_age_group['AgeGroup'] = pd.cut(
            df_with_age_group['Age'], 
            bins=[0, 30, 50, 100], 
            labels=['Young', 'Middle', 'Senior']
        ).astype(str)
        
        # åˆ›å»ºåµŒå¥—åˆ†åŒºè¡¨
        nested_path = os.path.join(self.output_dir, 'nested_partitioned_table')
        if os.path.exists(nested_path):
            shutil.rmtree(nested_path)
        
        table = pa.Table.from_pandas(df_with_age_group)
        pq.write_to_dataset(
            table,
            root_path=nested_path,
            partition_cols=partition_cols
        )
        
        # åˆ†æåµŒå¥—åˆ†åŒºç»“æ„
        nested_info = self._analyze_nested_partitions(nested_path)
        
        print(f"åµŒå¥—åˆ†åŒºå·²åˆ›å»º: {nested_path}")
        print(f"åˆ†åŒºå±‚çº§: {len(partition_cols)}")
        print(f"å¶å­åˆ†åŒºæ•°: {nested_info['leaf_partitions']}")
        print(f"æ€»å¤§å°: {nested_info['total_size']:.2f} MB")
        
        # æµ‹è¯•åµŒå¥—åˆ†åŒºæŸ¥è¯¢
        def query_nested():
            import pyarrow.dataset as ds
            dataset = ds.dataset(nested_path, format="parquet")
            table = dataset.to_table()
            df = table.to_pandas()
            # æ·»åŠ å¹´é¾„æ®µåˆ—ç”¨äºæµ‹è¯•
            df['AgeGroup'] = df['Age'].apply(lambda x: 'Young' if x < 30 else ('Middle' if x < 50 else 'Senior'))
            return df[(df['City'] == 'Beijing') & (df['AgeGroup'] == 'Middle')] if 'City' in df.columns else df
        
        df_nested, nested_time = self.performance_analyzer.measure_time(query_nested)
        
        print(f"åµŒå¥—åˆ†åŒºæŸ¥è¯¢: {nested_time:.4f} ç§’, ç»“æœ: {len(df_nested)} è¡Œ")
        
        return {
            'partition_cols': partition_cols,
            'nested_info': nested_info,
            'query_time': nested_time,
            'result_rows': len(df_nested)
        }
    
    def _analyze_nested_partitions(self, path: str) -> Dict[str, Any]:
        """åˆ†æåµŒå¥—åˆ†åŒºç»“æ„"""
        leaf_partitions = 0
        total_size = 0
        
        for root, dirs, files in os.walk(path):
            for file in files:
                if file.endswith('.parquet'):
                    file_path = os.path.join(root, file)
                    size = self.performance_analyzer.get_file_size(file_path)
                    total_size += size
                    leaf_partitions += 1
        
        return {
            'leaf_partitions': leaf_partitions,
            'total_size': total_size
        }
    
    def run_partitioning_exercise(self) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„åˆ†åŒºç»ƒä¹ 
        
        Returns:
            æ‰€æœ‰åˆ†åŒºæµ‹è¯•çš„ç»“æœ
        """
        print("=" * 60)
        print("å¼€å§‹ Parquet åˆ†åŒºç»ƒä¹ ")
        print("=" * 60)
        
        results = {}
        
        # 1. åˆ›å»ºè¡¨
        self.create_non_partitioned_table()
        partition_info = self.create_partitioned_table()
        results['partition_info'] = partition_info
        
        # 2. æµ‹è¯•åˆ†åŒºè£å‰ª
        results['partition_pruning'] = self.test_partition_pruning()
        
        # 3. æµ‹è¯•å¤šç§æŸ¥è¯¢åœºæ™¯
        results['multiple_queries'] = self.test_multiple_partition_queries()
        
        # 4. åˆ†æåˆ†åŒºåˆ†å¸ƒ
        results['distribution_analysis'] = self.analyze_partition_distribution()
        
        # 5. æµ‹è¯•åµŒå¥—åˆ†åŒº
        results['nested_partitioning'] = self.test_nested_partitioning()
        
        # æ˜¾ç¤ºæ€»ç»“
        self.display_partitioning_summary(results)
        
        # ä¿å­˜ç»“æœ
        results_file = os.path.join(self.output_dir, 'partitioning_results.json')
        self.performance_analyzer.save_results(results, results_file)
        
        return results
    
    def display_partitioning_summary(self, results: Dict[str, Any]) -> None:
        """
        æ˜¾ç¤ºåˆ†åŒºæ€»ç»“
        
        Args:
            results: åˆ†åŒºæµ‹è¯•ç»“æœ
        """
        print("\n" + "=" * 60)
        print("åˆ†åŒºç»ƒä¹ æ€»ç»“")
        print("=" * 60)
        
        print("ğŸ¯ åˆ†åŒºæ•ˆæœ:")
        
        if 'partition_pruning' in results:
            speedup = results['partition_pruning'].get('speedup', 0)
            print(f"â€¢ åˆ†åŒºè£å‰ªæ€§èƒ½æå‡: {speedup:.2f}x")
        
        if 'partition_info' in results:
            partition_count = results['partition_info'].get('partition_count', 0)
            print(f"â€¢ åˆ†åŒºæ•°é‡: {partition_count}")
        
        print("\nğŸ’¡ åˆ†åŒºæœ€ä½³å®è·µ:")
        print("â€¢ é€‰æ‹©æŸ¥è¯¢é¢‘ç¹çš„åˆ—ä½œä¸ºåˆ†åŒºé”®")
        print("â€¢ é¿å…åˆ›å»ºè¿‡å¤šå°åˆ†åŒº")
        print("â€¢ è€ƒè™‘æ•°æ®åˆ†å¸ƒçš„å‡è¡¡æ€§")
        print("â€¢ åˆç†ä½¿ç”¨åµŒå¥—åˆ†åŒº")
        print("â€¢ å®šæœŸç›‘æ§åˆ†åŒºæ€§èƒ½")
    
    def cleanup(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        from .utils import cleanup_files
        patterns = [
            self.non_partitioned_path,
            self.partitioned_path,
            os.path.join(self.output_dir, 'nested_partitioned_table')
        ]
        cleanup_files(patterns)