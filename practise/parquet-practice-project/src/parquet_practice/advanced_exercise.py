"""
Parquet é«˜çº§ç‰¹æ€§ç»ƒä¹ æ¨¡å—

æä¾›åµŒå¥—æ•°æ®ã€å…ƒæ•°æ®æ“ä½œã€æµå¼å¤„ç†ç­‰é«˜çº§ç‰¹æ€§çš„æ¼”ç¤ºã€‚
"""

import pyarrow as pa
import pyarrow.parquet as pq
import pandas as pd
import numpy as np
import os
import json
from typing import Dict, Any, List, Optional, Iterator

from .utils import DataGenerator, PerformanceAnalyzer


class ParquetAdvancedExercise:
    """Parquet é«˜çº§ç‰¹æ€§ç»ƒä¹ ç±»"""
    
    def __init__(self, output_dir: str = "output"):
        """
        åˆå§‹åŒ–é«˜çº§ç‰¹æ€§ç»ƒä¹ 
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        self.output_dir = output_dir
        self.data_generator = DataGenerator()
        self.performance_analyzer = PerformanceAnalyzer()
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
    
    def test_nested_data_structures(self, num_records: int = 1000) -> Dict[str, Any]:
        """
        æµ‹è¯•åµŒå¥—æ•°æ®ç»“æ„
        
        Args:
            num_records: è®°å½•æ•°é‡
            
        Returns:
            åµŒå¥—æ•°æ®æµ‹è¯•ç»“æœ
        """
        print("=" * 60)
        print("æµ‹è¯•åµŒå¥—æ•°æ®ç»“æ„")
        print("=" * 60)
        
        # ç”ŸæˆåµŒå¥—æ•°æ®
        nested_data = self.data_generator.generate_nested_data(num_records)
        
        # è½¬æ¢ä¸º PyArrow Table
        table = pa.Table.from_pandas(nested_data)
        
        print("åµŒå¥—æ•°æ®ç»“æ„:")
        print(table.schema)
        
        # ä¿å­˜åµŒå¥—æ•°æ®
        nested_file = os.path.join(self.output_dir, 'nested_data.parquet')
        
        def write_nested():
            pq.write_table(table, nested_file)
        
        _, write_time = self.performance_analyzer.measure_time(write_nested)
        
        print(f"å†™å…¥åµŒå¥—æ•°æ®: {write_time:.4f} ç§’")
        
        # è¯»å–åµŒå¥—æ•°æ®
        def read_nested():
            return pq.read_table(nested_file).to_pandas()
        
        df_read, read_time = self.performance_analyzer.measure_time(read_nested)
        
        print(f"è¯»å–åµŒå¥—æ•°æ®: {read_time:.4f} ç§’")
        
        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        original_rows = len(nested_data)
        read_rows = len(df_read)
        data_integrity = original_rows == read_rows
        
        print(f"æ•°æ®å®Œæ•´æ€§: {'âœ“' if data_integrity else 'âœ—'}")
        print(f"åŸå§‹è¡Œæ•°: {original_rows}, è¯»å–è¡Œæ•°: {read_rows}")
        
        # åˆ†æåµŒå¥—åˆ—
        nested_columns = []
        for col in df_read.columns:
            if df_read[col].dtype == 'object':
                sample_value = df_read[col].iloc[0]
                if isinstance(sample_value, (list, dict)):
                    nested_columns.append(col)
        
        print(f"åµŒå¥—åˆ—: {nested_columns}")
        
        # æ–‡ä»¶å¤§å°
        file_size = self.performance_analyzer.get_file_size(nested_file)
        print(f"æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
        
        return {
            'write_time': write_time,
            'read_time': read_time,
            'data_integrity': data_integrity,
            'original_rows': original_rows,
            'read_rows': read_rows,
            'nested_columns': nested_columns,
            'file_size_mb': file_size
        }
    
    def test_metadata_operations(self) -> Dict[str, Any]:
        """
        æµ‹è¯•å…ƒæ•°æ®æ“ä½œ
        
        Returns:
            å…ƒæ•°æ®æ“ä½œæµ‹è¯•ç»“æœ
        """
        print("\n" + "=" * 60)
        print("æµ‹è¯•å…ƒæ•°æ®æ“ä½œ")
        print("=" * 60)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        df = self.data_generator.generate_user_data(1000)
        table = pa.Table.from_pandas(df)
        
        # æ·»åŠ è‡ªå®šä¹‰å…ƒæ•°æ®
        metadata = {
            'created_by': 'Parquet Practice Exercise',
            'version': '1.0',
            'description': 'Test data for metadata operations',
            'schema_version': '2023.1'
        }
        
        # å°†å…ƒæ•°æ®æ·»åŠ åˆ° schema
        existing_metadata = table.schema.metadata or {}
        updated_metadata = {**existing_metadata}
        for key, value in metadata.items():
            updated_metadata[key.encode()] = value.encode()
        
        schema_with_metadata = table.schema.with_metadata(updated_metadata)
        table_with_metadata = table.cast(schema_with_metadata)
        
        # ä¿å­˜å¸¦å…ƒæ•°æ®çš„æ–‡ä»¶
        metadata_file = os.path.join(self.output_dir, 'with_metadata.parquet')
        pq.write_table(table_with_metadata, metadata_file)
        
        print("å·²æ·»åŠ è‡ªå®šä¹‰å…ƒæ•°æ®:")
        for key, value in metadata.items():
            print(f"â€¢ {key}: {value}")
        
        # è¯»å–å¹¶æ£€æŸ¥å…ƒæ•°æ®
        parquet_file = pq.ParquetFile(metadata_file)
        
        print("\næ–‡ä»¶å…ƒæ•°æ®ä¿¡æ¯:")
        print(f"â€¢ è¡Œæ•°: {parquet_file.metadata.num_rows:,}")
        print(f"â€¢ åˆ—æ•°: {parquet_file.metadata.num_columns}")
        print(f"â€¢ è¡Œç»„æ•°: {parquet_file.metadata.num_row_groups}")
        print(f"â€¢ åˆ›å»ºè€…: {parquet_file.metadata.created_by or 'Unknown'}")
        
        # è¯»å–è‡ªå®šä¹‰å…ƒæ•°æ®
        custom_metadata = {}
        try:
            # å°è¯•ä» schema çš„ pandas metadata ä¸­è·å–è‡ªå®šä¹‰å…ƒæ•°æ®
            schema_metadata = parquet_file.schema_arrow.metadata
            if schema_metadata:
                for key, value in schema_metadata.items():
                    try:
                        key_str = key.decode() if isinstance(key, bytes) else str(key)
                        value_str = value.decode() if isinstance(value, bytes) else str(value)
                        if not key_str.startswith('pandas'):  # è·³è¿‡ pandas å†…éƒ¨å…ƒæ•°æ®
                            custom_metadata[key_str] = value_str
                    except (UnicodeDecodeError, AttributeError):
                        pass
        except AttributeError:
            # å¦‚æœæ²¡æœ‰å…ƒæ•°æ®ï¼Œç»§ç»­æ‰§è¡Œ
            pass
        
        print("\nè‡ªå®šä¹‰å…ƒæ•°æ®:")
        for key, value in custom_metadata.items():
            print(f"â€¢ {key}: {value}")
        
        # åˆ—çº§å…ƒæ•°æ®
        print("\nåˆ—ä¿¡æ¯:")
        for i, column in enumerate(parquet_file.schema):
            print(f"â€¢ åˆ— {i}: {column.name} ({column.physical_type})")
        
        return {
            'file_metadata': {
                'num_rows': parquet_file.metadata.num_rows,
                'num_columns': parquet_file.metadata.num_columns,
                'num_row_groups': parquet_file.metadata.num_row_groups,
                'created_by': parquet_file.metadata.created_by or 'Unknown'
            },
            'custom_metadata': custom_metadata,
            'schema_info': [
                {'name': col.name, 'type': str(col.physical_type)}
                for col in parquet_file.schema
            ]
        }
    
    def test_streaming_operations(self, total_records: int = 10000, batch_size: int = 1000) -> Dict[str, Any]:
        """
        æµ‹è¯•æµå¼æ“ä½œ
        
        Args:
            total_records: æ€»è®°å½•æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            
        Returns:
            æµå¼æ“ä½œæµ‹è¯•ç»“æœ
        """
        print("\n" + "=" * 60)
        print("æµ‹è¯•æµå¼æ“ä½œ")
        print("=" * 60)
        
        streaming_file = os.path.join(self.output_dir, 'streaming_data.parquet')
        
        # æµå¼å†™å…¥
        print(f"æµå¼å†™å…¥ {total_records:,} æ¡è®°å½• (æ‰¹æ¬¡å¤§å°: {batch_size:,})")
        
        def streaming_write():
            writer = None
            batches_written = 0
            
            try:
                for i in range(0, total_records, batch_size):
                    # ç”Ÿæˆæ‰¹æ¬¡æ•°æ®
                    current_batch_size = min(batch_size, total_records - i)
                    batch_df = self.data_generator.generate_user_data(current_batch_size)
                    batch_table = pa.Table.from_pandas(batch_df)
                    
                    if writer is None:
                        # åˆ›å»ºå†™å…¥å™¨
                        writer = pq.ParquetWriter(streaming_file, batch_table.schema)
                    
                    # å†™å…¥æ‰¹æ¬¡
                    writer.write_table(batch_table)
                    batches_written += 1
                    
                    if batches_written % 5 == 0:
                        print(f"å·²å†™å…¥ {batches_written} ä¸ªæ‰¹æ¬¡...")
                
            finally:
                if writer:
                    writer.close()
            
            return batches_written
        
        batches_written, write_time = self.performance_analyzer.measure_time(streaming_write)
        
        print(f"æµå¼å†™å…¥å®Œæˆ: {write_time:.4f} ç§’, {batches_written} ä¸ªæ‰¹æ¬¡")
        
        # æµå¼è¯»å–
        print("\næµå¼è¯»å–æ•°æ®:")
        
        def streaming_read():
            parquet_file = pq.ParquetFile(streaming_file)
            total_rows = 0
            batches_read = 0
            
            for batch in parquet_file.iter_batches(batch_size=batch_size):
                batch_df = batch.to_pandas()
                total_rows += len(batch_df)
                batches_read += 1
                
                if batches_read % 5 == 0:
                    print(f"å·²è¯»å– {batches_read} ä¸ªæ‰¹æ¬¡, {total_rows:,} è¡Œ...")
            
            return total_rows, batches_read
        
        (total_rows, batches_read), read_time = self.performance_analyzer.measure_time(streaming_read)
        
        print(f"æµå¼è¯»å–å®Œæˆ: {read_time:.4f} ç§’")
        print(f"æ€»è¡Œæ•°: {total_rows:,}, æ‰¹æ¬¡æ•°: {batches_read}")
        
        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        data_integrity = total_rows == total_records
        print(f"æ•°æ®å®Œæ•´æ€§: {'âœ“' if data_integrity else 'âœ—'}")
        
        # æ–‡ä»¶å¤§å°
        file_size = self.performance_analyzer.get_file_size(streaming_file)
        print(f"æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
        
        return {
            'total_records': total_records,
            'batch_size': batch_size,
            'write_time': write_time,
            'read_time': read_time,
            'batches_written': batches_written,
            'batches_read': batches_read,
            'total_rows_read': total_rows,
            'data_integrity': data_integrity,
            'file_size_mb': file_size
        }
    
    def test_schema_evolution(self) -> Dict[str, Any]:
        """
        æµ‹è¯• Schema æ¼”è¿›
        
        Returns:
            Schema æ¼”è¿›æµ‹è¯•ç»“æœ
        """
        print("\n" + "=" * 60)
        print("æµ‹è¯• Schema æ¼”è¿›")
        print("=" * 60)
        
        # åˆ›å»ºåˆå§‹ schema
        initial_data = pd.DataFrame({
            'id': range(100),
            'name': [f'User_{i}' for i in range(100)],
            'age': np.random.randint(18, 80, 100)
        })
        
        initial_file = os.path.join(self.output_dir, 'schema_v1.parquet')
        initial_table = pa.Table.from_pandas(initial_data)
        pq.write_table(initial_table, initial_file)
        
        print("åˆå§‹ Schema (v1):")
        print(initial_table.schema)
        
        # æ¼”è¿› schema - æ·»åŠ æ–°åˆ—
        evolved_data = initial_data.copy()
        evolved_data['email'] = [f'user_{i}@example.com' for i in range(100)]
        evolved_data['city'] = np.random.choice(['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·'], 100)
        
        evolved_file = os.path.join(self.output_dir, 'schema_v2.parquet')
        evolved_table = pa.Table.from_pandas(evolved_data)
        pq.write_table(evolved_table, evolved_file)
        
        print("\næ¼”è¿›å Schema (v2):")
        print(evolved_table.schema)
        
        # æµ‹è¯•å…¼å®¹æ€§è¯»å–
        print("\næµ‹è¯• Schema å…¼å®¹æ€§:")
        
        # è¯»å– v1 æ–‡ä»¶
        v1_table = pq.read_table(initial_file)
        print(f"v1 æ–‡ä»¶åˆ—æ•°: {len(v1_table.schema)}")
        
        # è¯»å– v2 æ–‡ä»¶
        v2_table = pq.read_table(evolved_file)
        print(f"v2 æ–‡ä»¶åˆ—æ•°: {len(v2_table.schema)}")
        
        # å°è¯•ç”¨ v1 schema è¯»å– v2 æ–‡ä»¶çš„éƒ¨åˆ†åˆ—
        try:
            v2_partial = pq.read_table(evolved_file, columns=['id', 'name', 'age'])
            compatibility_test = len(v2_partial.schema) == len(v1_table.schema)
            print(f"å‘åå…¼å®¹æ€§: {'âœ“' if compatibility_test else 'âœ—'}")
        except Exception as e:
            print(f"å‘åå…¼å®¹æ€§: âœ— ({str(e)})")
            compatibility_test = False
        
        # åˆå¹¶ä¸åŒ schema çš„æ•°æ®
        print("\nåˆå¹¶ä¸åŒ Schema çš„æ•°æ®:")
        try:
            # ä¸º v1 æ•°æ®æ·»åŠ ç¼ºå¤±åˆ—
            v1_df = v1_table.to_pandas()
            v1_df['email'] = None
            v1_df['city'] = None
            
            v2_df = v2_table.to_pandas()
            
            # åˆå¹¶æ•°æ®
            combined_df = pd.concat([v1_df, v2_df], ignore_index=True)
            
            combined_file = os.path.join(self.output_dir, 'schema_combined.parquet')
            combined_table = pa.Table.from_pandas(combined_df)
            pq.write_table(combined_table, combined_file)
            
            print(f"åˆå¹¶æˆåŠŸ: {len(combined_df)} è¡Œ, {len(combined_df.columns)} åˆ—")
            merge_success = True
            
        except Exception as e:
            print(f"åˆå¹¶å¤±è´¥: {str(e)}")
            merge_success = False
        
        return {
            'initial_schema': str(initial_table.schema),
            'evolved_schema': str(evolved_table.schema),
            'v1_columns': len(v1_table.schema),
            'v2_columns': len(v2_table.schema),
            'backward_compatibility': compatibility_test,
            'merge_success': merge_success
        }
    
    def test_data_types_and_encoding(self) -> Dict[str, Any]:
        """
        æµ‹è¯•æ•°æ®ç±»å‹å’Œç¼–ç 
        
        Returns:
            æ•°æ®ç±»å‹å’Œç¼–ç æµ‹è¯•ç»“æœ
        """
        print("\n" + "=" * 60)
        print("æµ‹è¯•æ•°æ®ç±»å‹å’Œç¼–ç ")
        print("=" * 60)
        
        # åˆ›å»ºåŒ…å«å„ç§æ•°æ®ç±»å‹çš„æ•°æ®
        data = {
            'int8_col': np.random.randint(-128, 127, 1000, dtype=np.int8),
            'int16_col': np.random.randint(-32768, 32767, 1000, dtype=np.int16),
            'int32_col': np.random.randint(-2147483648, 2147483647, 1000, dtype=np.int32),
            'int64_col': np.random.randint(-9223372036854775808, 9223372036854775807, 1000, dtype=np.int64),
            'float32_col': np.random.random(1000).astype(np.float32),
            'float64_col': np.random.random(1000).astype(np.float64),
            'bool_col': np.random.choice([True, False], 1000),
            'string_col': [f'String_{i}' for i in range(1000)],
            'category_col': np.random.choice(['A', 'B', 'C', 'D'], 1000),
            'datetime_col': pd.date_range('2023-01-01', periods=1000, freq='H'),
            'decimal_col': np.round(np.random.random(1000) * 1000, 2)
        }
        
        df = pd.DataFrame(data)
        
        # è½¬æ¢ä¸º PyArrow è¡¨å¹¶æŒ‡å®šç²¾ç¡®çš„æ•°æ®ç±»å‹
        schema = pa.schema([
            ('int8_col', pa.int8()),
            ('int16_col', pa.int16()),
            ('int32_col', pa.int32()),
            ('int64_col', pa.int64()),
            ('float32_col', pa.float32()),
            ('float64_col', pa.float64()),
            ('bool_col', pa.bool_()),
            ('string_col', pa.string()),
            ('category_col', pa.dictionary(pa.int32(), pa.string())),
            ('datetime_col', pa.timestamp('ns')),
            ('decimal_col', pa.float64())  # ä½¿ç”¨ float64 è€Œä¸æ˜¯ decimal128
        ])
        
        table = pa.Table.from_pandas(df, schema=schema)
        
        print("æ•°æ®ç±»å‹ Schema:")
        for field in table.schema:
            print(f"â€¢ {field.name}: {field.type}")
        
        # ä¿å­˜æ–‡ä»¶
        types_file = os.path.join(self.output_dir, 'data_types.parquet')
        pq.write_table(table, types_file)
        
        # è¯»å–å¹¶éªŒè¯ç±»å‹
        read_table = pq.read_table(types_file)
        
        print("\nè¯»å–åçš„æ•°æ®ç±»å‹:")
        for field in read_table.schema:
            print(f"â€¢ {field.name}: {field.type}")
        
        # ç±»å‹ä¿æŒæ€§æ£€æŸ¥
        type_preservation = str(table.schema) == str(read_table.schema)
        print(f"\næ•°æ®ç±»å‹ä¿æŒæ€§: {'âœ“' if type_preservation else 'âœ—'}")
        
        # æ–‡ä»¶å¤§å°åˆ†æ
        file_size = self.performance_analyzer.get_file_size(types_file)
        print(f"æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
        
        # åˆ†æç¼–ç æ•ˆæœ
        parquet_file = pq.ParquetFile(types_file)
        encoding_info = []
        
        for rg in range(parquet_file.metadata.num_row_groups):
            row_group = parquet_file.metadata.row_group(rg)
            for col in range(row_group.num_columns):
                column = row_group.column(col)
                encoding_info.append({
                    'column': column.path_in_schema,
                    'encoding': str(column.encodings),
                    'compression': str(column.compression),
                    'total_byte_size': getattr(column, 'total_byte_size', 0),
                    'total_compressed_size': getattr(column, 'total_compressed_size', 0)
                })
        
        print("\nç¼–ç ä¿¡æ¯:")
        for info in encoding_info[:5]:  # æ˜¾ç¤ºå‰5åˆ—çš„ä¿¡æ¯
            compression_ratio = info['total_byte_size'] / info['total_compressed_size'] if info['total_compressed_size'] > 0 else 1
            print(f"â€¢ {info['column']}: {info['encoding']}, å‹ç¼©æ¯”: {compression_ratio:.2f}")
        
        return {
            'schema': str(table.schema),
            'type_preservation': type_preservation,
            'file_size_mb': file_size,
            'encoding_info': encoding_info
        }
    
    def run_advanced_exercise(self) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„é«˜çº§ç‰¹æ€§ç»ƒä¹ 
        
        Returns:
            æ‰€æœ‰é«˜çº§ç‰¹æ€§æµ‹è¯•çš„ç»“æœ
        """
        print("=" * 60)
        print("å¼€å§‹ Parquet é«˜çº§ç»ƒä¹ ")
        print("=" * 60)
        
        results = {}
        
        # 1. Nested data structure test
        results['nested_data'] = self.test_nested_data_structures()
        
        # 2. Metadata operations test
        results['metadata'] = self.test_metadata_operations()
        
        # 3. Streaming operations test
        results['streaming'] = self.test_streaming_operations()
        
        # 4. Schema evolution test
        results['schema_evolution'] = self.test_schema_evolution()
        
        # 5. Data types and encoding test
        results['data_types'] = self.test_data_types_and_encoding()
        
        # 6. Compression algorithm test
        results['compression'] = self.test_compression_algorithms()
        
        # Display summary
        self.display_advanced_summary(results)
        
        # ä¿å­˜ç»“æœ
        results_file = os.path.join(self.output_dir, 'advanced_results.json')
        self.performance_analyzer.save_results(results, results_file)
        
        return results
    
    def test_compression_algorithms(self) -> Dict[str, Any]:
        """
        æµ‹è¯•å‹ç¼©ç®—æ³•å¯¹æ¯”
        
        Returns:
            å‹ç¼©ç®—æ³•æµ‹è¯•ç»“æœ
        """
        print("\n" + "=" * 60)
        print("æµ‹è¯•å‹ç¼©ç®—æ³•å¯¹æ¯”")
        print("=" * 60)
        
        compression_types = ['snappy', 'gzip', 'brotli', 'lz4']
        results = {}
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        test_data = self.data_generator.generate_user_data(5000)
        table = pa.Table.from_pandas(test_data)
        
        for compression in compression_types:
            print(f"\næµ‹è¯•å‹ç¼©ç®—æ³•: {compression}")
            
            filename = os.path.join(self.output_dir, f'data_{compression}.parquet')
            
            # ä¿å­˜
            def save_with_compression():
                pq.write_table(table, filename, compression=compression)
            
            _, save_time = self.performance_analyzer.measure_time(save_with_compression)
            
            # è¯»å–
            def read_compressed():
                return pq.read_table(filename)
            
            _, read_time = self.performance_analyzer.measure_time(read_compressed)
            
            # æ–‡ä»¶å¤§å°
            file_size = self.performance_analyzer.get_file_size(filename)
            
            results[compression] = {
                'save_time': save_time,
                'read_time': read_time,
                'file_size': file_size
            }
            
            print(f"ä¿å­˜æ—¶é—´: {save_time:.4f} ç§’")
            print(f"è¯»å–æ—¶é—´: {read_time:.4f} ç§’")
            print(f"æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
        
        # æ‰¾å‡ºæœ€ä½³å‹ç¼©ç®—æ³•
        best_compression = min(results.keys(), key=lambda x: results[x]['file_size'])
        fastest_save = min(results.keys(), key=lambda x: results[x]['save_time'])
        fastest_read = min(results.keys(), key=lambda x: results[x]['read_time'])
        
        print(f"\nå‹ç¼©ç®—æ³•å¯¹æ¯”ç»“æœ:")
        print(f"â€¢ æœ€ä½³å‹ç¼©ç‡: {best_compression} ({results[best_compression]['file_size']:.2f} MB)")
        print(f"â€¢ æœ€å¿«ä¿å­˜: {fastest_save} ({results[fastest_save]['save_time']:.4f} ç§’)")
        print(f"â€¢ æœ€å¿«è¯»å–: {fastest_read} ({results[fastest_read]['read_time']:.4f} ç§’)")
        
        return {
            'compression_results': results,
            'best_compression': best_compression,
            'fastest_save': fastest_save,
            'fastest_read': fastest_read
        }
    
    def display_advanced_summary(self, results: Dict[str, Any]) -> None:
        """
        æ˜¾ç¤ºé«˜çº§ç‰¹æ€§æ€»ç»“
        
        Args:
            results: é«˜çº§ç‰¹æ€§æµ‹è¯•ç»“æœ
        """
        print("\n" + "=" * 60)
        print("é«˜çº§ç‰¹æ€§ç»ƒä¹ æ€»ç»“")
        print("=" * 60)
        
        print("ğŸ¯ é«˜çº§ç‰¹æ€§éªŒè¯:")
        
        if 'nested_data' in results:
            integrity = results['nested_data'].get('data_integrity', False)
            print(f"â€¢ åµŒå¥—æ•°æ®å¤„ç†: {'âœ“' if integrity else 'âœ—'}")
        
        if 'metadata' in results:
            metadata_count = len(results['metadata'].get('custom_metadata', {}))
            print(f"â€¢ å…ƒæ•°æ®æ“ä½œ: âœ“ ({metadata_count} ä¸ªè‡ªå®šä¹‰å­—æ®µ)")
        
        if 'streaming' in results:
            streaming_integrity = results['streaming'].get('data_integrity', False)
            print(f"â€¢ æµå¼å¤„ç†: {'âœ“' if streaming_integrity else 'âœ—'}")
        
        if 'schema_evolution' in results:
            compatibility = results['schema_evolution'].get('backward_compatibility', False)
            print(f"â€¢ Schema æ¼”è¿›: {'âœ“' if compatibility else 'âœ—'}")
        
        if 'data_types' in results:
            type_preservation = results['data_types'].get('type_preservation', False)
            print(f"â€¢ æ•°æ®ç±»å‹ä¿æŒ: {'âœ“' if type_preservation else 'âœ—'}")
        
        if 'compression' in results:
            best_compression = results['compression'].get('best_compression', 'N/A')
            print(f"â€¢ å‹ç¼©ç®—æ³•æµ‹è¯•: âœ“ (æœ€ä½³: {best_compression})")
        
        print("\nğŸ’¡ é«˜çº§ç‰¹æ€§åº”ç”¨åœºæ™¯:")
        print("â€¢ åµŒå¥—æ•°æ®: JSONã€XML ç­‰åŠç»“æ„åŒ–æ•°æ®å­˜å‚¨")
        print("â€¢ å…ƒæ•°æ®: æ•°æ®è¡€ç¼˜ã€ç‰ˆæœ¬æ§åˆ¶ã€è´¨é‡æ ‡è®°")
        print("â€¢ æµå¼å¤„ç†: å¤§æ•°æ®é›†çš„å†…å­˜å‹å¥½å¤„ç†")
        print("â€¢ Schema æ¼”è¿›: æ•°æ®æ¨¡å‹çš„å¹³æ»‘å‡çº§")
        print("â€¢ æ•°æ®ç±»å‹: ç²¾ç¡®çš„æ•°æ®è¡¨ç¤ºå’Œå­˜å‚¨ä¼˜åŒ–")
        print("â€¢ å‹ç¼©ç®—æ³•: å­˜å‚¨ç©ºé—´ä¸æ€§èƒ½çš„å¹³è¡¡")
    
    def cleanup(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        from .utils import cleanup_files
        patterns = [
            os.path.join(self.output_dir, 'nested_data.parquet'),
            os.path.join(self.output_dir, 'with_metadata.parquet'),
            os.path.join(self.output_dir, 'streaming_data.parquet'),
            os.path.join(self.output_dir, 'schema_v1.parquet'),
            os.path.join(self.output_dir, 'schema_v2.parquet'),
            os.path.join(self.output_dir, 'schema_combined.parquet'),
            os.path.join(self.output_dir, 'data_types.parquet'),
            os.path.join(self.output_dir, 'data_*.parquet')
        ]
        cleanup_files(patterns)