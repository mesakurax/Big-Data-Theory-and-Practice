#!/usr/bin/env python3
"""
Parquet æ€§èƒ½åŸºå‡†æµ‹è¯•

è¿™ä¸ªè„šæœ¬æä¾›äº†å…¨é¢çš„ Parquet æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼Œ
åŒ…æ‹¬ä¸åŒæ•°æ®é‡ã€ä¸åŒå‹ç¼©ç®—æ³•çš„æ€§èƒ½å¯¹æ¯”ã€‚
"""

import os
import sys
import time
import pandas as pd
import matplotlib.pyplot as plt
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, os.path.join(project_root, 'src'))

from parquet_practice import (
    DataGenerator, 
    ParquetCompressionExercise,
    PerformanceAnalyzer
)


class ParquetBenchmark:
    """Parquet æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    def __init__(self, output_dir: str = "benchmark_results"):
        """
        åˆå§‹åŒ–åŸºå‡†æµ‹è¯•
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        self.output_dir = output_dir
        self.data_generator = DataGenerator()
        self.performance_analyzer = PerformanceAnalyzer()
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        print("ğŸ¯ Parquet æ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("=" * 50)
    
    def benchmark_data_sizes(self, sizes: List[int] = None) -> Dict[str, Any]:
        """
        æµ‹è¯•ä¸åŒæ•°æ®é‡çš„æ€§èƒ½
        
        Args:
            sizes: æ•°æ®é‡åˆ—è¡¨
            
        Returns:
            æµ‹è¯•ç»“æœ
        """
        if sizes is None:
            sizes = [1000, 5000, 10000, 50000, 100000]
        
        print("ğŸ“Š æµ‹è¯•ä¸åŒæ•°æ®é‡çš„æ€§èƒ½...")
        
        results = {
            'sizes': sizes,
            'parquet_write_times': [],
            'parquet_read_times': [],
            'csv_write_times': [],
            'csv_read_times': [],
            'parquet_sizes': [],
            'csv_sizes': []
        }
        
        for size in sizes:
            print(f"  æµ‹è¯• {size:,} æ¡è®°å½•...")
            
            # ç”Ÿæˆæ•°æ®
            data = self.data_generator.generate_user_data(size)
            df = pd.DataFrame(data)
            
            # æ–‡ä»¶è·¯å¾„
            parquet_file = os.path.join(self.output_dir, f"benchmark_{size}.parquet")
            csv_file = os.path.join(self.output_dir, f"benchmark_{size}.csv")
            
            # æµ‹è¯• Parquet å†™å…¥
            start_time = time.time()
            df.to_parquet(parquet_file, compression='snappy')
            parquet_write_time = time.time() - start_time
            results['parquet_write_times'].append(parquet_write_time)
            
            # æµ‹è¯• CSV å†™å…¥
            start_time = time.time()
            df.to_csv(csv_file, index=False)
            csv_write_time = time.time() - start_time
            results['csv_write_times'].append(csv_write_time)
            
            # æµ‹è¯• Parquet è¯»å–
            start_time = time.time()
            pd.read_parquet(parquet_file)
            parquet_read_time = time.time() - start_time
            results['parquet_read_times'].append(parquet_read_time)
            
            # æµ‹è¯• CSV è¯»å–
            start_time = time.time()
            pd.read_csv(csv_file)
            csv_read_time = time.time() - start_time
            results['csv_read_times'].append(csv_read_time)
            
            # æ–‡ä»¶å¤§å°
            parquet_size = self.performance_analyzer.get_file_size(parquet_file)
            csv_size = self.performance_analyzer.get_file_size(csv_file)
            results['parquet_sizes'].append(parquet_size)
            results['csv_sizes'].append(csv_size)
            
            # æ¸…ç†æ–‡ä»¶
            os.remove(parquet_file)
            os.remove(csv_file)
        
        return results
    
    def benchmark_compression_algorithms(self, num_records: int = 10000) -> Dict[str, Any]:
        """
        æµ‹è¯•ä¸åŒå‹ç¼©ç®—æ³•çš„æ€§èƒ½
        
        Args:
            num_records: è®°å½•æ•°é‡
            
        Returns:
            æµ‹è¯•ç»“æœ
        """
        print(f"ğŸ—œï¸ æµ‹è¯•ä¸åŒå‹ç¼©ç®—æ³•çš„æ€§èƒ½ ({num_records:,} æ¡è®°å½•)...")
        
        # ç”Ÿæˆæ•°æ®
        data = self.data_generator.generate_user_data(num_records)
        
        # åˆ›å»ºå‹ç¼©ç»ƒä¹ å®ä¾‹
        exercise = ParquetCompressionExercise(data, self.output_dir)
        
        # è¿è¡Œå‹ç¼©æµ‹è¯•
        results = exercise.run_compression_exercise()
        
        # æ¸…ç†
        exercise.cleanup()
        
        return results
    
    def plot_size_benchmark(self, results: Dict[str, Any]) -> None:
        """ç»˜åˆ¶æ•°æ®é‡åŸºå‡†æµ‹è¯•å›¾è¡¨"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        sizes = results['sizes']
        
        # å†™å…¥æ—¶é—´å¯¹æ¯”
        ax1.plot(sizes, results['parquet_write_times'], 'o-', label='Parquet', color='blue')
        ax1.plot(sizes, results['csv_write_times'], 's-', label='CSV', color='red')
        ax1.set_xlabel('è®°å½•æ•°é‡')
        ax1.set_ylabel('å†™å…¥æ—¶é—´ (ç§’)')
        ax1.set_title('å†™å…¥æ€§èƒ½å¯¹æ¯”')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # è¯»å–æ—¶é—´å¯¹æ¯”
        ax2.plot(sizes, results['parquet_read_times'], 'o-', label='Parquet', color='blue')
        ax2.plot(sizes, results['csv_read_times'], 's-', label='CSV', color='red')
        ax2.set_xlabel('è®°å½•æ•°é‡')
        ax2.set_ylabel('è¯»å–æ—¶é—´ (ç§’)')
        ax2.set_title('è¯»å–æ€§èƒ½å¯¹æ¯”')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # æ–‡ä»¶å¤§å°å¯¹æ¯”
        ax3.plot(sizes, [s/1024/1024 for s in results['parquet_sizes']], 'o-', label='Parquet', color='blue')
        ax3.plot(sizes, [s/1024/1024 for s in results['csv_sizes']], 's-', label='CSV', color='red')
        ax3.set_xlabel('è®°å½•æ•°é‡')
        ax3.set_ylabel('æ–‡ä»¶å¤§å° (MB)')
        ax3.set_title('æ–‡ä»¶å¤§å°å¯¹æ¯”')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # è¯»å–é€Ÿåº¦æå‡å€æ•°
        speedups = [csv_time / parquet_time for csv_time, parquet_time in 
                   zip(results['csv_read_times'], results['parquet_read_times'])]
        ax4.plot(sizes, speedups, 'o-', color='green')
        ax4.set_xlabel('è®°å½•æ•°é‡')
        ax4.set_ylabel('é€Ÿåº¦æå‡å€æ•°')
        ax4.set_title('Parquet è¯»å–é€Ÿåº¦æå‡')
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        chart_path = os.path.join(self.output_dir, 'size_benchmark.png')
        plt.savefig(chart_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š æ•°æ®é‡åŸºå‡†æµ‹è¯•å›¾è¡¨å·²ä¿å­˜: {chart_path}")
        
        plt.show()
    
    def run_full_benchmark(self) -> None:
        """è¿è¡Œå®Œæ•´çš„åŸºå‡†æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹å®Œæ•´åŸºå‡†æµ‹è¯•...")
        
        # 1. æ•°æ®é‡åŸºå‡†æµ‹è¯•
        size_results = self.benchmark_data_sizes()
        
        # 2. å‹ç¼©ç®—æ³•åŸºå‡†æµ‹è¯•
        compression_results = self.benchmark_compression_algorithms()
        
        # 3. ç”ŸæˆæŠ¥å‘Š
        self.generate_benchmark_report(size_results, compression_results)
        
        # 4. ç»˜åˆ¶å›¾è¡¨
        self.plot_size_benchmark(size_results)
        
        print("âœ… å®Œæ•´åŸºå‡†æµ‹è¯•å®Œæˆï¼")
    
    def generate_benchmark_report(self, size_results: Dict[str, Any], 
                                compression_results: Dict[str, Any]) -> None:
        """ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š"""
        report_path = os.path.join(self.output_dir, 'benchmark_report.md')
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# Parquet æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š\n\n")
            f.write(f"æµ‹è¯•æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # æ•°æ®é‡æµ‹è¯•ç»“æœ
            f.write("## æ•°æ®é‡æ€§èƒ½æµ‹è¯•\n\n")
            f.write("| è®°å½•æ•°é‡ | Parquetå†™å…¥(s) | CSVå†™å…¥(s) | Parquetè¯»å–(s) | CSVè¯»å–(s) | è¯»å–æå‡ | Parquetå¤§å°(MB) | CSVå¤§å°(MB) | å‹ç¼©ç‡ |\n")
            f.write("|---------|---------------|-----------|---------------|-----------|---------|----------------|-------------|--------|\n")
            
            for i, size in enumerate(size_results['sizes']):
                parquet_write = size_results['parquet_write_times'][i]
                csv_write = size_results['csv_write_times'][i]
                parquet_read = size_results['parquet_read_times'][i]
                csv_read = size_results['csv_read_times'][i]
                speedup = csv_read / parquet_read
                parquet_size = size_results['parquet_sizes'][i] / 1024 / 1024
                csv_size = size_results['csv_sizes'][i] / 1024 / 1024
                compression_ratio = (1 - parquet_size / csv_size) * 100
                
                f.write(f"| {size:,} | {parquet_write:.3f} | {csv_write:.3f} | {parquet_read:.3f} | {csv_read:.3f} | {speedup:.1f}x | {parquet_size:.2f} | {csv_size:.2f} | {compression_ratio:.1f}% |\n")
            
            # å‹ç¼©ç®—æ³•æµ‹è¯•ç»“æœ
            f.write("\n## å‹ç¼©ç®—æ³•æ€§èƒ½æµ‹è¯•\n\n")
            if 'compression_results' in compression_results:
                f.write("| å‹ç¼©ç®—æ³• | å†™å…¥æ—¶é—´(s) | è¯»å–æ—¶é—´(s) | æ–‡ä»¶å¤§å°(MB) | å‹ç¼©æ¯” |\n")
                f.write("|---------|------------|------------|-------------|--------|\n")
                
                for result in compression_results['compression_results']:
                    algorithm = result['algorithm']
                    write_time = result['write_time']
                    read_time = result['read_time']
                    file_size = result['file_size'] / 1024 / 1024
                    compression_ratio = result['compression_ratio']
                    
                    f.write(f"| {algorithm} | {write_time:.3f} | {read_time:.3f} | {file_size:.2f} | {compression_ratio:.1f} |\n")
            
            # æ€»ç»“
            f.write("\n## æµ‹è¯•æ€»ç»“\n\n")
            f.write("### ä¸»è¦å‘ç°\n\n")
            f.write("1. **è¯»å–æ€§èƒ½**: Parquet æ ¼å¼åœ¨è¯»å–æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äº CSV æ ¼å¼\n")
            f.write("2. **å­˜å‚¨æ•ˆç‡**: Parquet æ ¼å¼å…·æœ‰æ›´å¥½çš„å‹ç¼©æ•ˆæœï¼ŒèŠ‚çœå­˜å‚¨ç©ºé—´\n")
            f.write("3. **å‹ç¼©ç®—æ³•**: ä¸åŒå‹ç¼©ç®—æ³•åœ¨æ€§èƒ½å’Œå‹ç¼©æ¯”ä¹‹é—´æœ‰ä¸åŒçš„æƒè¡¡\n")
            f.write("4. **æ‰©å±•æ€§**: éšç€æ•°æ®é‡å¢åŠ ï¼ŒParquet çš„ä¼˜åŠ¿æ›´åŠ æ˜æ˜¾\n\n")
            
            f.write("### å»ºè®®\n\n")
            f.write("- å¯¹äºå¤§æ•°æ®åˆ†æåœºæ™¯ï¼Œæ¨èä½¿ç”¨ Parquet æ ¼å¼\n")
            f.write("- æ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©åˆé€‚çš„å‹ç¼©ç®—æ³•\n")
            f.write("- åœ¨æ•°æ®ç®¡é“ä¸­è€ƒè™‘ä½¿ç”¨ Parquet æå‡æ•´ä½“æ€§èƒ½\n")
        
        print(f"ğŸ“„ åŸºå‡†æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")


def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºåŸºå‡†æµ‹è¯•å®ä¾‹
    benchmark = ParquetBenchmark()
    
    # è¿è¡Œå®Œæ•´åŸºå‡†æµ‹è¯•
    benchmark.run_full_benchmark()


if __name__ == '__main__':
    main()